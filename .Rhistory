coef(lm.fit)
library(boot)
glm.fit=glm(mpg~horsepower ,data=Auto)
glm.fit=glm(mpg~horsepower ,data=Auto)
cv.err=cv.glm(Auto,glm.fit)
cv.err$delta
cv.error=rep(0,5)
for (i in 1:5){
glm.fit=glm(mpg~poly(horsepower ,i),data=Auto)
cv.error[i]=cv.glm(Auto,glm.fit)$delta[1]
}
cv.error
set.seed(17)
cv.error.10=rep(0,10)
for (i in 1:10){
glm.fit=glm(mpg~poly(horsepower ,i),data=Auto)
cv.error.10[i]=cv.glm(Auto,glm.fit,K=10)$delta[1]
}
cv.error.10
df = as.data.frame(cbind(x,y))
set.seed(1994)
errors = rep(0,4)
loocverr = rep(0,100)
for(i in 1:4){
for (j in 1:100){
train = df[-j,]
model8 =  glm(y~ poly(x,i), data = train)
loocverr[j] =cv.glm(Auto,glm.fit)$delta[1]
}
error[i] <- mean(loocverr)
}
set.seed(1)
y=rnorm(100)
x=rnorm(100)
y=x-2*x^2+rnorm(100)
plot(x,y)
df = as.data.frame(cbind(x,y))
set.seed(1994)
errors = rep(0,4)
loocverr = rep(0,100)
for(i in 1:4){
for (j in 1:100){
train = df[-j,]
model8 =  glm(y~ poly(x,i), data = train)
loocverr[j] =cv.glm(Auto,glm.fit)$delta[1]
}
error[i] <- mean(loocverr)
}
set.seed(1)
y=rnorm(100)
x=rnorm(100)
y=x-2*x^2+rnorm(100)
plot(x,y)
df = as.data.frame(cbind(x,y))
set.seed(1994)
errors = rep(0,4)
for(i in 1:4){
model8 =  glm(y~ poly(x,i), data = df)
errors[i] =cv.glm(Auto,model8)$delta[1]
}
error
errors
errors[i] =cv.glm(Auto,model8)$delta
warnings()
df = as.data.frame(cbind(x,y))
set.seed(1994)
errors = rep(0,4)
df = as.data.frame(cbind(x,y))
set.seed(1994)
errors = rep(0,4)
for(i in 1:4){
model8 =  glm(y~ poly(x,i), data = df)
errors[i] =cv.glm(Auto,model8)$delta
}
errors
warnings()
errors = rep(0,4)
for(i in 1:4){
model8 =  glm(y~ poly(x,i), data = df)
errors[i] =cv.glm(df,model8)$delta
}
errors = rep(0,4)
for(i in 1:4){
model8 =  glm(y~ poly(x,i), data = df)
errors[i] =cv.glm(df,model8)$delta
}
errors
set.seed(1227)
errors = rep(0,4)
for(i in 1:4){
model8 =  glm(y~ poly(x,i), data = df)
errors[i] =cv.glm(df,model8)$delta[1]
}
errors
alpha.fn=function(data,index){
X=data$X[index]
Y=data$Y[index]
return((var(Y)-cov(X,Y))/(var(X)+var(Y)-2*cov(X,Y)))
}
alpha.fn(Portfolio,1:100)
set.seed(1)
alpha.fn(Portfolio,sample(100,100, replace = T))
boot(portfolio,alpha.fn,R=1000)
boot(Portfolio,alpha.fn,R=1000)
boot.fn=function(data,index) return(coef(lm(mpg∼horsepower ,data=data,subset=index)))
boot.fn=function(data,index) return(coef(lm(mpg~horsepower ,data=data,subset=index)))
boot.fn(Auto ,1:392)
set.seed (1)
boot.fn(Auto,sample(392,392,replace=T))
boot.fn(Auto,sample(392,392,replace=T))
boot(Auto, boot.fn, R = 1000)
summary(lm(mpg∼horsepower ,data=Auto))$coef
summary(lm(mpg~horsepower ,data=Auto))$coef
boot.fn=function(data,index) coefficients(lm(mpg~horsepower+I(horsepower^2),data=data,subset=index))
set.seed (1)
boot(Auto ,boot.fn ,1000)
summary(lm(mpg~horsepower+I(horsepower^2),data=Auto))$coef
library(tm)
library(readr)
setwd("~/Desktop/School/SYS6018/sys6018-competition-twitter-sentiment")
train <- read_csv("train.csv")
train <- read_csv("train.csv")
news = VCorpus(DataframeSource(train))
tweets = VCorpus(DataframeSource(train))
inspect(tweets[1:2])
tweets[[1]]
tweets[[1]]$content
tweets.tfidf = DocumentTermMatrix(tweets, control = list(weighting = weightTfIdf))
tweets.tfidf[1:5,1:5]
tweets.tfidf
as.matrix(tweets.tfidf[1:5,1:5])
as.matrix(tweets.tfidf)
stopwords("english")
tweets.clean = tm_map(tweets, stripWhitespace)                          # remove extra whitespace
tweets.clean = tm_map(tweets.clean, removeNumbers)                      # remove numbers
tweets.clean = tm_map(tweets.clean, removePunctuation)                  # remove punctuation
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
tweets.clean
as.matrix(tweets.clean)
tweets.clean
tweets.clean[[1]]$content
?tm_map
library(tm)
library(readr)
train <- read_csv("train.csv")
tweets = VCorpus(DataframeSource(train))
inspect(tweets[1:2])
tweets[[1]]$content
?gsub
tweets.clean = tm_map(tweets, stripWhitespace)                          # remove extra whitespace
tweets.clean = tm_map(tweets.clean, removeNumbers)                      # remove numbers
f <- content_transformer(function(x, pattern) gsub(pattern, "", x))
tweets.clean = tm_map(tweets.clean, f,"@[:alpha]+")
tweets.clean = tm_map(tweets.clean, removePunctuation)                  # remove punctuation
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
tweets.clean[[1]]$content
tweets.clean = tm_map(tweets, stripWhitespace)                          # remove extra whitespace
tweets.clean = tm_map(tweets.clean, removeNumbers)                      # remove numbers
f <- content_transformer(function(x, pattern) gsub(pattern, "", x))
tweets.clean = tm_map(tweets.clean, f,"@[[:alpha]]+")
tweets.clean = tm_map(tweets.clean, f,"@[:alpha]+"[[1]])
tweets.clean = tm_map(tweets.clean, removePunctuation)                  # remove punctuation
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
tweets.clean[[1]]$content
tweets.clean = tm_map(tweets, stripWhitespace)                          # remove extra whitespace
tweets.clean = tm_map(tweets.clean, removeNumbers)                      # remove numbers
f <- content_transformer(function(x, pattern) gsub(pattern, "", x))
tweets.clean = tm_map(tweets.clean, f,"@[:alpha]+"[[1]])
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
tweets.clean[[1]]$content
tweets.clean = tm_map(tweets, stripWhitespace)                          # remove extra whitespace
tweets.clean = tm_map(tweets.clean, removeNumbers)                      # remove numbers
f <- content_transformer(function(x, pattern) gsub(pattern, "", x))
tweets.clean = tm_map(tweets.clean, f,"@[:alpha:]+"[[1]])
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
tweets.clean[[1]]$content
tweets.clean = tm_map(tweets, stripWhitespace)                          # remove extra whitespace
tweets.clean = tm_map(tweets.clean, removeNumbers)                      # remove numbers
f <- content_transformer(function(x, pattern) gsub(pattern, "", x))
tweets.clean = tm_map(tweets.clean, f,"@([:alpha:]+)")
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
tweets.clean[[1]]$content
f <- content_transformer(function(x, pattern) gsub(pattern, "", x))
tweets.clean = tm_map(tweets.clean, f,"@([a-z]+)")
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
tweets.clean[[1]]$content
tweets.clean = tm_map(tweets.clean, f,"@(\s+)")
tweets.clean = tm_map(tweets.clean, f,"@(\\s+)")
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
tweets.clean[[1]]$content
tweets.clean = tm_map(tweets, stripWhitespace)                          # remove extra whitespace
tweets.clean = tm_map(tweets.clean, removeNumbers)                      # remove numbers
f <- content_transformer(function(x, pattern) gsub(pattern, "", x))
tweets.clean = tm_map(tweets.clean, f,"@(\\s+)")
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
tweets.clean[[1]]$content
tweets.clean = tm_map(tweets.clean, f,"@([:alnum:])")
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
tweets.clean[[1]]$content
tweets.clean = tm_map(tweets.clean, f,"@[:alnum:]")
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
tweets.clean[[1]]$content
tweets.clean = tm_map(tweets.clean, f,"@[:alnum:]+")
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
tweets.clean[[1]]$content
tweets.clean = tm_map(tweets.clean, f,"@[\\s]+")
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
tweets.clean[[1]]$content
tweets.clean = tm_map(tweets.clean, f,"@[\\S]+")
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
tweets.clean[[1]]$content
tweets.clean = tm_map(tweets.clean, f,"@[[\\S]]+")
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
tweets.clean[[1]]$content
tweets.clean = tm_map(tweets.clean, f,"@[[:alnum:]]+")
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
tweets.clean[[1]]$content
tweets.clean = tm_map(tweets, stripWhitespace)                          # remove extra whitespace
tweets.clean = tm_map(tweets.clean, removeNumbers)                      # remove numbers
f <- content_transformer(function(x, pattern) gsub(pattern, "", x))
tweets.clean = tm_map(tweets.clean, f,"@[[:alnum:]]+")
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
tweets.clean[[1]]$content
tweets.clean = tm_map(tweets, stripWhitespace)                          # remove extra whitespace
tweets.clean = tm_map(tweets.clean, removeNumbers)                      # remove numbers
f <- content_transformer(function(x, pattern) gsub(pattern, "", x))
tweets.clean = tm_map(tweets.clean, f,"@[[^:space:]]+")
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
tweets.clean[[1]]$content
tweets.clean = tm_map(tweets.clean, f,"@[[\\S]]+")
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
tweets.clean[[1]]$content
tweets.clean = tm_map(tweets, stripWhitespace)                          # remove extra whitespace
tweets.clean = tm_map(tweets.clean, removeNumbers)                      # remove numbers
f <- content_transformer(function(x, pattern) gsub(pattern, "", x))
tweets.clean = tm_map(tweets.clean, f,"@[[\\S]]+")
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
tweets.clean[[1]]$content
tweets.clean = tm_map(tweets.clean, f,"@\\S+")
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
tweets.clean[[1]]$content
tweets.clean = tm_map(tweets, stripWhitespace)                          # remove extra whitespace
tweets.clean = tm_map(tweets.clean, removeNumbers)                      # remove numbers
f <- content_transformer(function(x, pattern) gsub(pattern, "", x))
tweets.clean = tm_map(tweets.clean, f,"@\\S+")
tweets.clean = tm_map(tweets.clean, removePunctuation)                  # remove punctuation
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
tweets.clean[[1]]$content
tweets.clean = tm_map(tweets.clean, removeNumbers)                      # remove numbers
f <- content_transformer(function(x, pattern) gsub(pattern, "", x))     # build regex function
tweets.clean = tm_map(tweets.clean, f,"@\\S+")                          # remove usernames
tweets.clean = tm_map(tweets.clean, removePunctuation)                  # remove punctuation
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
tweets.clean = tm_map(tweets, stripWhitespace)                          # remove extra whitespace
tweets.clean[[1]]$content
tweets.clean = tm_map(tweets.clean, f,"@\\S+")                          # remove usernames
tweets.clean = tm_map(tweets.clean, removePunctuation)                  # remove punctuation
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
tweets.clean[[1]]$content
tweets.clean = tm_map(tweets, stripWhitespace)                          # remove extra whitespace
tweets.clean[[1]]$content
tweets.clean = tm_map(tweets, removeNumbers)                      # remove numbers
f <- content_transformer(function(x, pattern) gsub(pattern, "", x))     # build regex function
tweets.clean = tm_map(tweets.clean, f,"@\\S+")                          # remove usernames
tweets.clean = tm_map(tweets.clean, removePunctuation)                  # remove punctuation
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
tweets.clean = tm_map(tweets.clean, stripWhitespace)                          # remove extra whitespace
tweets.clean[[1]]$content
tweets.clean[[1]]$content
tweets.clean = tm_map(tweets.clean, f,"@\\S+")                          # remove usernames
tweets.clean = tm_map(tweets.clean, stripWhitespace)                          # remove extra whitespace
tweets.clean = tm_map(tweets.clean, removePunctuation)                  # remove punctuation
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
tweets.clean[[1]]$content
tweets.clean[[1]]
View(train)
tweets.clean[[1]]
tweets.clean[[1]]$content
tweets.clean[[1]]$content
tweets[[1]]$content
tweets.clean[[1]]$content
tweets.clean = tm_map(tweets, removeNumbers)                      # remove numbers
f <- content_transformer(function(x, pattern) gsub(pattern, "", x))     # build regex function
tweets.clean = tm_map(tweets.clean, f,"@\\S+"[[1]])                          # remove usernames
tweets.clean = tm_map(tweets.clean, stripWhitespace)                          # remove extra whitespace
tweets.clean = tm_map(tweets.clean, removePunctuation)                  # remove punctuation
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
tweets[[1]]$content
tweets.clean[[1]]$content
tweets.clean = tm_map(tweets[[1]], removeNumbers)                      # remove numbers
tweets[[1]]$content
tweets.clean[[1]]$content
tweets.clean.tfidf = DocumentTermMatrix(tweets, control = list(weighting = weightTfIdf))
tweets.clean.tfidf
as.matrix(tweets.clean.tfidf)
as.matrix(tweets.clean.tfidf)[1:5]
as.matrix(tweets.clean.tfidf)[1:5,1:5]
tweets.clean.tfidf = DocumentTermMatrix(tweets.clean, control = list(weighting = weightTfIdf))
tweets.clean.tfidf
as.matrix(tweets.clean.tfidf)[1:5,1:5]
as.matrix(tweets.clean.tfidf)
tweets.90 = removeSparseTerms(tweets.clean.tfidf, 0.9)  # remove terms that are absent from at least 99% of documents (keep most terms)
tfidf.90
tweets.90
as.matrix(tweets.99[1:5,1:5])
as.matrix(tweets.90[1:5,1:5])
as.matrix(tweets.90)
tweets.90 = removeSparseTerms(tweets.clean.tfidf, 0.7)  # remove terms that are absent from at least 99% of documents (keep most terms)
tweets.90
as.matrix(tweets.90)
tweets.90 = removeSparseTerms(tweets.clean.tfidf, 0.999)  # remove terms that are absent from at least 99% of documents (keep most terms)
tweets.90
as.matrix(tweets.90)
tweets.90 = removeSparseTerms(tweets.clean.tfidf, 0.99)  # remove terms that are absent from at least 99% of documents (keep most terms)
tweets.90
as.matrix(tweets.90)
tweets.90 = removeSparseTerms(tweets.clean.tfidf, 0.95)  # remove terms that are absent from at least 99% of documents (keep most terms)
tweets.90
as.matrix(tweets.90)
tweets.90 = removeSparseTerms(tweets.clean.tfidf, 0.97)  # remove terms that are absent from at least 99% of documents (keep most terms)
tweets.90
as.matrix(tweets.90)
tweets.90
tweets.90 = removeSparseTerms(tweets.clean.tfidf, 0.99)  # remove terms that are absent from at least 99% of documents (keep most terms)
tweets.90
as.matrix(tweets.90)
as.matrix(tweets.90)
tweets.train = as.matrix(tweets.90)
View(tweets.train)
cbind(tweets.train,train$sentiment)
?cbind
View(tweets.train)
train.new = cbind(tweets.train,train$sentiment)
View(train.new)
tweets.train = as.matrix(tweets.90)
train.new = cbind(tweets.train,train$sentiment)
tweets.train = as.data.frame(tweets.90)
tweets.train = as.data.frame(as.matrix(tweets.90))
train.new = cbind(tweets.train,train$sentiment)
View(train.new)
View(train.new)
class(train.new)
model <- lm(`train$sentiment`~.,data = train.new)
summary(model)
test  <-  read_csv("test.csv")
g <- function(train){
tweets = VCorpus(DataframeSource(train))
inspect(tweets[1:2])
tweets[[1]]$content
tweets.clean = tm_map(tweets, removeNumbers)                      # remove numbers
f <- content_transformer(function(x, pattern) gsub(pattern, "", x))     # build regex function
tweets.clean = tm_map(tweets.clean, f,"@\\S+")                          # remove usernames
tweets.clean = tm_map(tweets.clean, stripWhitespace)                          # remove extra whitespace
tweets.clean = tm_map(tweets.clean, removePunctuation)                  # remove punctuation
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)
tweets[[1]]$content
tweets.clean[[1]]$content
tweets.clean.tfidf = DocumentTermMatrix(tweets.clean, control = list(weighting = weightTfIdf))
tweets.clean.tfidf
return(as.matrix(tweets.clean.tfidf))
}
tweets.90 = g(train)
tweets.90 = removeSparseTerms(tweets.clean.tfidf, 0.99)  # remove terms that are absent from at least 99% of documents (keep most terms)
tweets.90
tweets.train = as.data.frame(as.matrix(tweets.90))
train.new = cbind(tweets.train,train$sentiment)
test.90 <- g(test)
test.90 <- as.data.frame(g(test))
predict(model,data = test.90)
guesses  = predict(model,data = test.90)
guesses
guesses = round(guesses,digits = 0)
guesses
View(test)
guesses = cbind(test$id,guesses)
guesses  = predict(model,data = test.90)
test.90 <- as.data.frame(g(test))
guesses  = predict(model,data = test.90)
?predict
guesses  = predict.lm(model,newdata = test.90)
summary(model)
model <- lm(`train$sentiment`~.-fbi,data = train.new)
summary(model)
guesses  = predict.lm(model,newdata = test.90)
model <- lm(`train$sentiment`~.-fbi,data = train.new)
summary(model)
guesses  = predict.lm(model,newdata = test.90)
model <- lm(`train$sentiment`~-fbi,data = train.new)
summary(model)
guesses  = predict.lm(model,newdata = test.90)
model <- lm(`train$sentiment`~-fbi,data = train.new)
summary(model)
model <- lm(`train$sentiment`~.-fbi,data = train.new)
summary(model)
guesses  = predict.lm(model,newdata = test.90)
test.90 <- as.data.frame(g(test))
model <- lm(`train$sentiment`~.-fbi,data = train.new)
summary(model)
guesses  = predict.lm(model,newdata = test.90)
View(test.90)
model <- lm(`train$sentiment`~. -fbi,data = train.new)
summary(model)
guesses  = predict.lm(model,newdata = test.90)
model <- lm(`train$sentiment`~. - fbi ,data = train.new)
summary(model)
guesses  = predict.lm(model,newdata = test.90)
model2 <- lm(`train$sentiment`~. - fbi ,data = train.new)
guesses  = predict.lm(model2,newdata = test.90)
guesses  = predict(model2,newdata = test.90)
summary(model)
View(test.90)
train.new$fbi <-  NULL
model2 <- lm(`train$sentiment`~. - fbi ,data = train.new)
model2 <- lm(`train$sentiment`~.,data = train.new)
summary(model)
guesses  = predict(model2,newdata = test.90)
train.new$januari <-  NULL
model2 <- lm(`train$sentiment`~.,data = train.new)
summary(model)
guesses  = predict(model2,newdata = test.90)
train.new$univers <-  NULL
model2 <- lm(`train$sentiment`~.,data = train.new)
summary(model)
guesses  = predict(model2,newdata = test.90)
guesses = round(guesses,digits = 0)
guesses = cbind(test$id,guesses)
View(guesses)
colnames(guesses) <- c("id","sentiment")
View(guesses)
write.csv("sentiment_guesses")
write.csv(guesses,"sentiment_guesses")
write.csv(guesses,"sentiment_guesses",row.names = F)
write.csv(guesses,"sentiment_guesses.csv",row.names = F)
?knn.reg
??knn.reg
library(FNN)
model3 <-  knn.reg
model3 <-  knn.reg(train = train.new)
model3 <-  knn.reg(train = train.new, test = test.90, y = `train$sentiment`)
test.90[col.names(test.90) %in% col.names(train.new)]
test.90[colnames(test.90) %in% colnames(train.new)]
test.90 = test.90[colnames(test.90) %in% colnames(train.new)]
model3 <-  knn.reg(train = train.new, test = test.90, y = `train$sentiment`)
model3 <-  knn.reg(train = train.new, test = test.90, y = `train$sentiment`)
train.new2 = train.new
train.new2$`train$sentiment` = NULL
model3 <-  knn.reg(train = train.new2, test = test.90, y = `train$sentiment`)
library(RWeka)
install.packages("RWeka")
library(RWeka)
library(RWeka)
install.packages("rJava")
library(RWeka)
tweets.90 = removeSparseTerms(tweets.clean.tfidf, 0.99)  # remove terms that are absent from at least 99% of documents (keep most terms)
tweets.90 = g(train)
tweets.90 = removeSparseTerms(tweets.90, 0.99)  # remove terms that are absent from at least 99% of documents (keep most terms)
